{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2184de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from .base import Tokenizer, get_stats, merge\n",
    "\n",
    "\n",
    "# the main GPT text split patterns, see\n",
    "# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n",
    "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "\n",
    "class RegexTokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self, pattern=None):\n",
    "        \"\"\"\n",
    "        - pattern: optional string to override the default (GPT-4 split pattern)\n",
    "        - special_tokens: str -> int dictionary of special tokens\n",
    "          example: {'<|endoftext|>': 100257}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n",
    "        self.compiled_pattern = re.compile(self.pattern)\n",
    "        self.special_tokens = {}\n",
    "        self.inverse_special_tokens = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66f4291",
   "metadata": {},
   "source": [
    "I’ll explain **only** this part in very simple English, step by step, with small advanced-word explanations.\n",
    "\n",
    "---\n",
    "\n",
    "## `re.compile(self.pattern)` — simple meaning\n",
    "\n",
    "This line **turns the regex (regular expression) string into a “compiled pattern object.”**\n",
    "\n",
    "Beginner meaning:\n",
    "\n",
    "It **prepares the regex** so Python can run it fast later.\n",
    "\n",
    "When you use a regex many times (like in a tokenizer), Python normally must:\n",
    "\n",
    "* read the regex string\n",
    "* parse it\n",
    "* convert it to an internal format\n",
    "* then run it\n",
    "\n",
    "That is slow.\n",
    "\n",
    "`re.compile()` makes Python **do the parsing only once**.\n",
    "\n",
    "Then later, you can use:\n",
    "\n",
    "* `self.compiled_pattern.findall(text)`\n",
    "* `self.compiled_pattern.split(text)`\n",
    "* `self.compiled_pattern.search(text)`\n",
    "\n",
    "These run much faster.\n",
    "\n",
    "---\n",
    "\n",
    "## Think of it like a “pre-built machine”\n",
    "\n",
    "Imagine the regex pattern is a **blueprint**, and `re.compile` builds the **machine** from the blueprint.\n",
    "\n",
    "After the machine is built, using it is fast.\n",
    "\n",
    "---\n",
    "\n",
    "## What exactly happens inside Python?\n",
    "\n",
    "Here is a simple, clear breakdown.\n",
    "\n",
    "1. You give Python a regex string.\n",
    "   Example: `r\"\\p{L}+\"`\n",
    "\n",
    "2. Python reads the string character by character.\n",
    "\n",
    "3. Python converts it into an internal structure (like bytecode for regex).\n",
    "   Beginner explanation: It makes a small “program” that can match text.\n",
    "\n",
    "4. Python returns a **compiled regex object**.\n",
    "   Example: `<regex.Pattern object>`\n",
    "\n",
    "5. This object is stored in `self.compiled_pattern`.\n",
    "\n",
    "From now on, the tokenizer will use the **compiled** version, not the raw string.\n",
    "\n",
    "---\n",
    "\n",
    "## Why the tokenizer needs it\n",
    "\n",
    "The tokenizer will call:\n",
    "\n",
    "# re.findall(self.compiled_pattern, text)\n",
    "\n",
    "\n",
    "many, many times.\n",
    "\n",
    "If the pattern is not compiled, Python would compile it **every time**, making tokenization slow.\n",
    "\n",
    "So:\n",
    "\n",
    "* `re.compile` = compile once, use many times\n",
    "* Faster tokenization\n",
    "* Less CPU work\n",
    "* More stable behavior\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9d93f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6536ff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids, counts=None):\n",
    "    \"\"\"\n",
    "    Given a list of integers, return a dictionary of counts of consecutive pairs\n",
    "    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
    "    Optionally allows to update an existing dictionary of counts\n",
    "    \"\"\"\n",
    "    counts = {} if counts is None else counts\n",
    "    for pair in zip(ids, ids[1:]): # iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "688d17f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    \"\"\"\n",
    "    In the list of integers (ids), replace all consecutive occurrences\n",
    "    of pair with the new integer token idx\n",
    "    Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n",
    "    \"\"\"\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if not at the very last position AND the pair matches, replace it\n",
    "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed73b6d3",
   "metadata": {},
   "source": [
    "# if vocab_size less then 256 assert raise assertion error.\n",
    "\n",
    "# for example: vocab_size = 200 -----> 200 >= 256 it is False\n",
    "\n",
    "# num_merges used for how many times for cycle has to work\n",
    "\n",
    "# if vocab_size = 300 it means we have 44 new words which needs to take new ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f76dce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_chunks: ['Hello', ',', ' world', '!']\n",
      "ids: [[72, 101, 108, 108, 111], [44], [32, 119, 111, 114, 108, 100], [33]]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT use: import re\n",
    "import regex as re  # <--- This is the key change\n",
    "\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "# This will now compile successfully because the 'regex' library supports \\p\n",
    "compiled_pattern = re.compile(GPT4_SPLIT_PATTERN)\n",
    "\n",
    "text = \"Hello, world!\"\n",
    "\n",
    "text_chunks = re.findall(compiled_pattern, text)\n",
    "# used downloaded re compile\n",
    "\n",
    "\n",
    "ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n",
    "\n",
    "print(f\"text_chunks: {text_chunks}\")\n",
    "print(f\"ids: {ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d9375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def train_bpe(text, vocab_size, verbose=False):\n",
    "    \"\"\"\n",
    "    Standalone version of RegexTokenizer.train\n",
    "    Returns: (merges, vocab)\n",
    "    \"\"\"\n",
    "    # 1. Configuration\n",
    "    assert vocab_size >= 256\n",
    "    \n",
    "    num_merges = vocab_size - 256\n",
    "    GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "    compiled_pattern = re.compile(GPT4_SPLIT_PATTERN)\n",
    "    \n",
    "    print(f\"[Info] Starting training. Target merges: {num_merges}\")\n",
    "    \n",
    "    text_chunks = re.findall(compiled_pattern, text)  # example: ['Hello', ',', ' world', '!']\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"[Info] Text split into {len(text_chunks)} chunks: {text_chunks}\")\n",
    "        \n",
    "    merges = {} # (int, int) -> int\n",
    "    vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
