{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e50dcd",
   "metadata": {},
   "source": [
    "GPT modellarida quydagi ketma-ketlikda ishlaydi:\n",
    "1.So'zlar avval tokenlarga(sonlarga) o'zgartiriladi\n",
    "tokenlarga o'tkazishda turli xil usullar bor misol uchun: word-level,character-level and subword-level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c742d55",
   "metadata": {},
   "source": [
    "# Word-level tokenization: Splits text into whole words (e.g., \"Let's learn tokenization.\" → [\"Let's\", \"learn\", \"tokenization\", \".\"]).\n",
    "Pros: Meaningful units, small sequences. Cons: Large vocabulary, poor handling of rare/OOV words, misspellings.\n",
    "# Character-level tokenization: Splits into individual characters (e.g., same sentence → [\"L\", \"e\", \"t\", \"'\", \"s\", ...]).\n",
    "Pros: Tiny vocabulary, handles any word/typos. Cons: Very long sequences, loses word meaning.\n",
    "# Subword-level tokenization: Splits into units between words and characters (e.g., BPE/WordPiece: \"tokenization\" → [\"token\", \"##ization\"]).\n",
    "Pros: Balanced vocabulary (~30k-50k), handles rare words, common in models like BERT/GPT. Cons: Slightly complex training.\n",
    "Subword examples: BPE (GPT), WordPiece (BERT), SentencePiece (T5, language-independent)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f20488",
   "metadata": {},
   "source": [
    "# So'z darajasidagi tokenizatsiya: Matnni butun so'zlarga bo'ladi (masalan, \"Let's learn tokenization.\" → [\"Let's\", \"learn\", \"tokenization\", \".\"]).  \n",
    "Afzalliklari: Ma'noli birliklar, qisqa ketma-ketliklar.  \n",
    "Kamchiliklari: Katta lug'at, kam uchraydigan/OOV so'zlar va xatolarni yomon boshqaradi.\n",
    "\n",
    "# Belgi darajasidagi tokenizatsiya: Har bir belgiga bo'ladi (shu jumla → [\"L\", \"e\", \"t\", \"'\", \"s\", ...]).  \n",
    "Afzalliklari: Kichik lug'at, har qanday so'z/xatolarni qamrab oladi.  \n",
    "Kamchiliklari: Juda uzun ketma-ketliklar, so'z ma'nosini yo'qotadi.\n",
    "\n",
    "# Subword darajasidagi tokenizatsiya: So'z va belgi orasidagi birliklarga bo'ladi (masalan, BPE/WordPiece: \"tokenization\" → [\"token\", \"##ization\"]).  \n",
    "Afzalliklari: Muvozanatli lug'at (~30k-50k), kam uchraydigan so'zlarni yaxshi boshqaradi, BERT/GPT kabi modellarda keng ishlatiladi.  \n",
    "Kamchiliklari: O'qitish biroz murakkab.\n",
    "\n",
    "Subword misollari: BPE (GPT), WordPiece (BERT), SentencePiece (T5, tillardan mustaqil)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744d97f6",
   "metadata": {},
   "source": [
    "GPT modellarida quydagi ketma-ketlikda ishlaydi:\n",
    "1.So'zlar avval tokenlarga(sonlarga) o'zgartiriladi\n",
    "tokenlarga o'tkazishda turli xil usullar bor misol uchun: word-level,character-level and subword-level\n",
    "2.So'zlar tokenga o'tkanidan so'ng tokenlar --> vektor emabddinga o'tadi."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
