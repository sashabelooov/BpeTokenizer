{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0686d159",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "                                                    Building GPT step by step\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf44c73",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "1.Taking all tokens and convert them into tokens\n",
    "for example: This is an example text ----> [123, 45, 50215, 25896, 7]\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ee486f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "For converting text into tokens ids there are several methods like Bpe, WordsentencePiece and so on\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4040bbea",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "2.After converting text into token ids,create Dataset and Dataloader for input-output targets\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073359a7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset and Dataloader example\n",
    "[6942] ----> 10730\n",
    "[6942, 10730] ----> 1634\n",
    "[6942, 10730, 1634] ----> 11\n",
    "[6942, 10730, 1634, 11] ----> 198"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255b8e37",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "3.Created tokens convert into vector emabddings.Why does it need ? Because giving differen kinds of question see more in Lecture_10/image-12.More value for question for example if question dog ----> has a tail ? if value big gpt can understand what does this word mean.\n",
    "Also vector can capture semantic meaning of the words.In additional gpt also should understand words meaning so for this we have to create vector emabdding or it calls token emabddings.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0da36f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Each word vector embaddings can be large and large I mean if we have word computer and token of this token can be ----> 456 ----> \n",
    "[1.23 , -0.898965, 2.5671512, -1.2378562414 ..... (there are 768 numbers for token 456)] and so on for example for each word can gives 768 dimantional vector it mean for 456 token gives another 768 numbers but in vector representation\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb87666",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "for gpt2 there are 50257 tokens and 768 vactore it means that 50257 rows and 768 columns (50257*768)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94dce3e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "torch.nn.Embedding(4,5) ----> this means that 4 words and for each words give another 5 numbers\n",
    "tensor([[ 1.2753, -0.2010, -0.1606],\n",
    "        [-0.4015,  0.9666, -1.1481],\n",
    "        [-2.8400, -0.7849, -1.4096],\n",
    "        [ 0.9178,  1.5810,  1.3010],\n",
    "        [1.5608, -2.0949, 0.8319]], grad_fn=<EmbeddingBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa2810e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
